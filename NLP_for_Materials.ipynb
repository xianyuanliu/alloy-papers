{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Collate PDFs"
   ],
   "metadata": {
    "id": "QJW19k6wuMPP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üì• Download and Extract Demo Papers\n",
    "# @markdown This cell removes any existing alloy-papers folder, creates a fresh one,\n",
    "# @markdown downloads a zipped archive of demo papers from Google Drive using gdown,\n",
    "# @markdown and extracts the contents into the `alloy-papers/` directory.\n",
    "\n",
    "!rm -rf alloy-papers\n",
    "!mkdir alloy-papers\n",
    "!cd alloy-papers\n",
    "\n",
    "!pip install -q gdown\n",
    "!gdown --id 1DvejY9En4cZlMlCs3Wgwspmjwmd8a902 --output alloy-papers/demo_papers.zip\n",
    "!unzip alloy-papers/demo_papers.zip -d alloy-papers/"
   ],
   "metadata": {
    "id": "UeJmgW1-J4tW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert PDFs to raw text\n"
   ],
   "metadata": {
    "id": "uwHOvDWdujRL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üõ†Ô∏è Install Nougat from GitHub\n",
    "!pip install -q git+https://github.com/facebookresearch/nougat.git"
   ],
   "metadata": {
    "id": "sJeHsrMJ5zcw",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üïµÔ∏è Convert PDFs to raw text using Nougat\n",
    "\n",
    "# @markdown Please enter the paper PDF file name and the folder to store the output MMD file\n",
    "pdf_filename = \"alloy-papers/dummy_alloy_paper.pdf\"  # @param {type:\"string\"}\n",
    "output_folder = \"alloy-papers/nougat\"  # @param {type:\"string\"}\n",
    "\n",
    "# !nougat \"{pdf_filename}\" -o \"{output_folder}\" -m 0.1.0-base --no-skipping\n",
    "!nougat \"{pdf_filename}\" -o \"{output_folder}\" --no-skipping\n",
    "\n",
    "\n",
    "# Force cleanup\n",
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": true,
    "id": "V1edhN5U93Xf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üìÑ Preview the Extracted File\n",
    "# @markdown Check the extracted MMD file.\n",
    "# @markdown You can compare it with the PDF file.\n",
    "\n",
    "mmd_filename = \"alloy-papers/nougat/dummy_alloy_paper.mmd\"  # @param {type:\"string\"}\n",
    "\n",
    "with open(mmd_filename, \"r\") as file:\n",
    "    latex_content = file.read()\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(latex_content))\n"
   ],
   "metadata": {
    "id": "2pFSR9rQ0tOz",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean raw text\n"
   ],
   "metadata": {
    "id": "zWCQml9VuqJd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üßº Data Cleaning for MMD Files\n",
    "# @markdown Remove Acknowledgement and References.\n",
    "\n",
    "# @markdown If you want to further remove Abstract and Introduction, please uncomment the relevant code.\n",
    "\n",
    "input_folder = \"alloy-papers/nougat\"  # @param {type:\"string\"}\n",
    "output_folder = \"alloy-papers/cleaned\"  # @param {type:\"string\"}\n",
    "\n",
    "# --- Script starts here ---\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "def remove_abstract_introduction(text):\n",
    "    # Define patterns to match Abstract, Introduction, and Experiment sections\n",
    "    abstract_pattern = re.compile(r\"(?i)^#+\\s*([ivxlcdm\\d]*\\s*)?a\\s*bstract\\s*(.*?)\", re.DOTALL | re.MULTILINE)\n",
    "    introduction_pattern = re.compile(r\"(?i)^#+\\s*([ivxlcdm\\d]*\\s*)?introduction\\s*(.*?)\", re.DOTALL | re.MULTILINE)\n",
    "    experiment_pattern = re.compile(\n",
    "        r\"(?i)^#+\\s*([ivxlcdm\\d]*\\s*)?(experiment(?:al|s)?|materials and methods|methodology)\\s*(.*?)(?=#+|$)\", re.DOTALL | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    # Search for matches\n",
    "    abstract_matches = list(abstract_pattern.finditer(text))\n",
    "    introduction_match = re.search(introduction_pattern, text)\n",
    "    experiment_match = re.search(experiment_pattern, text)\n",
    "\n",
    "    # Check if multiple abstracts exist\n",
    "    if len(abstract_matches) > 1:\n",
    "        # If multiple abstracts exist, keep the last one\n",
    "        abstract_match = abstract_matches[-1]\n",
    "    elif len(abstract_matches) == 1:\n",
    "        abstract_match = abstract_matches[0]\n",
    "    else:\n",
    "        abstract_match = None\n",
    "\n",
    "    # Check if abstract exists\n",
    "    if abstract_match:\n",
    "        if introduction_match:\n",
    "            if experiment_match:\n",
    "                # If abstract, introduction and experiment exist, remove content before experiment\n",
    "                text = text[experiment_match.start() :]\n",
    "            else:\n",
    "                # If abstract and introduction exist but experiment does not exist, remove content before introduction\n",
    "                text = text[introduction_match.start() :]\n",
    "        else:\n",
    "            # If abstract exists but introduction and experiment do not exist, only remove content before\n",
    "            # abstract to avoid removing the useful content\n",
    "            text = text[abstract_match.start() :]\n",
    "    else:\n",
    "        if introduction_match:\n",
    "            if experiment_match:\n",
    "                # If abstract does not exist but introduction and experiment exist, remove content before experiment\n",
    "                text = text[experiment_match.start() :]\n",
    "            else:\n",
    "                # If abstract and experiment do not exist but introduction exists, only remove content before\n",
    "                # introduction\n",
    "                text = text[introduction_match.start() :]\n",
    "        else:\n",
    "            # If abstract, introduction and experiment do not exist, keep the content\n",
    "            text = text\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_references_acknowledgement(text):\n",
    "    # Remove 'References' section (any heading level, with optional numbering)\n",
    "    text = re.sub(\n",
    "        r\"^#{1,6}\\s*\\d*\\s*References\\b.*?(?=^#{1,6}\\s|\\Z)\",\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.DOTALL | re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    sections_to_remove = [\n",
    "        \"Acknowledgements\",\n",
    "        \"Acknowledgments\",\n",
    "        \"Conflicts of interest\",\n",
    "        \"Declaration of competing interest\",\n",
    "        \"Disclosure statement\",\n",
    "        \"Funding\",\n",
    "        \"Conflicts of Interest\",\n",
    "        \"Supporting Information\",\n",
    "        \"Author Contributions\",\n",
    "    ]\n",
    "\n",
    "    # Match any heading level (# to ######), optional numbering, section name, and its content\n",
    "    pattern = (\n",
    "            r\"^#{1,6}\\s*\\d*\\s*(?:\" +\n",
    "            \"|\".join(re.escape(s) for s in sections_to_remove) +\n",
    "            r\")\\b.*?(?=^#{1,6}\\s|\\Z)\"\n",
    "    )\n",
    "\n",
    "    text = re.sub(pattern, \"\", text, flags=re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repetitions(text, threshold=3):\n",
    "    pattern = re.compile(r'\\b(.+?)\\b(?:\\s+\\1\\b){{{},}}'.format(threshold - 1))\n",
    "    cleaned = pattern.sub(r'\\1', text)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def process_files(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_folder)):\n",
    "        if file_name.endswith(\".mmd\"):  # Assuming the articles are in text files\n",
    "            with open(os.path.join(input_folder, file_name), \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "\n",
    "            cleaned_text = remove_repetitions(text)\n",
    "            cleaned_text = remove_references_acknowledgement(cleaned_text)\n",
    "            cleaned_text = remove_abstract_introduction(cleaned_text)\n",
    "\n",
    "            output_file_name = f\"{file_name}\"\n",
    "            with open(os.path.join(output_folder, output_file_name), \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(cleaned_text)\n",
    "\n",
    "process_files(input_folder, output_folder)"
   ],
   "metadata": {
    "id": "OcEZJ5Jp0tMN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üìÑ Preview LaTeX from .mmd File\n",
    "# @markdown Check the cleaned MMD file again.\n",
    "\n",
    "mmd_filename = \"alloy-papers/cleaned/dummy_alloy_paper.mmd\"  # @param {type:\"string\"}\n",
    "\n",
    "with open(mmd_filename, \"r\") as file:\n",
    "    latex_content = file.read()\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(latex_content))"
   ],
   "metadata": {
    "id": "TgBHxA6yAav0",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LLM Prompting\n"
   ],
   "metadata": {
    "id": "XKbMZb_fuswA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üîê Login to Hugging Face\n",
    "# @markdown Install the Hugging Face Hub and follow the instruction to generate your Access Token.\n",
    "\n",
    "# @markdown  You need to have an HuggingFace account to get the Token.\n",
    "\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "print(\"Successful login.\")"
   ],
   "metadata": {
    "id": "9Ee2MgizF_Hn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üöÄ Install Transformers 4.49.0 & Restart Runtime\n",
    "# @markdown This will install a specific version of Hugging Face Transformers library, used for working with large language models.\n",
    "# @markdown After installation, the Colab runtime will restart automatically to apply the changes.\n",
    "# @markdown Please note this Transformer version doesn't work with Nougat, so you should restart the session if you want to run Nougat again.\n",
    "\n",
    "!pip install -q transformers==4.49.0\n",
    "!pip install -q bitsandbytes\n",
    "\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "metadata": {
    "collapsed": true,
    "id": "YNzRSLivPH3d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üîç Extract Alloy & Phase Info from MMD Files\n",
    "# @markdown This cell runs an LLM-based extraction pipeline on MMD files generated from academic PDFs.\n",
    "# @markdown It identifies alloy compositions, and phase information based on structured prompts,\n",
    "# @markdown then cleans and saves the output as JSON in the specified output folder.\n",
    "\n",
    "llm_model = \"meta-llama/Llama-3.2-3B-Instruct\" # @param {type:\"string\"}\n",
    "input_folder = \"alloy-papers/cleaned\" # @param {type:\"string\"}\n",
    "output_dir = \"alloy-papers/output\" # @param {type:\"string\"}\n",
    "max_len = \"120000\" # @param {type:\"string\"}\n",
    "quantize = \"\" # @param {type:\"string\"}\n",
    "\n",
    "# @markdown LLM model options are [meta-llama/Llama-3.2-1B-Instruct, meta-llama/Llama-3.2-3B-Instruct]\n",
    "\n",
    "\n",
    "# --- Script starts here ---\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def split(tokenizer, text, max_len):\n",
    "    chunks, c = [], []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        c.append(line)\n",
    "        if len(tokenizer.tokenize(\"\\n\".join(c))) > max_len:\n",
    "            chunks.append(\"\\n\".join(c[:-1]))\n",
    "            c = [line]\n",
    "    chunks.append(\"\\n\".join(c))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chat(generator, dialog):\n",
    "    terminators = [\n",
    "        generator.tokenizer.eos_token_id,\n",
    "        generator.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "    ]\n",
    "\n",
    "    outputs = generator(\n",
    "        dialog,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    pattern = re.compile(r\"\\{.*?\\}\", re.DOTALL)\n",
    "    match = pattern.findall(text)\n",
    "    cleaned = \"[\" + \",\".join(match) + \"]\" if match else \"[]\"\n",
    "    try:\n",
    "        json.loads(cleaned)\n",
    "        return cleaned\n",
    "    except json.JSONDecodeError:\n",
    "        return \"[]\"\n",
    "\n",
    "def prompt_1(text):\n",
    "    return f\"\"\"You are a helpful assistant. Your task is to extract alloy and its corresponding phase information from the text.\n",
    "\n",
    "    Please follow these instructions carefully:\n",
    "\n",
    "    **Extract alloy and phase information** from the paper text, delimited by ‚Äî‚Äî‚Äî‚Äî‚Äî.\n",
    "        Please use the following JSON schema to generate the output:\n",
    "    {{\n",
    "       \"Alloy\": \"XXX\",\n",
    "       \"Phase\": \"XXX\"\n",
    "    }}\n",
    "\n",
    "    \"Alloy\": Elemental composition of the alloy mentioned in the paper. Alloy compositions contain multiple elements. Ignore alloys that include unknown placeholders (e.g., \"X\", \"Y\", or invalid symbols).\n",
    "    \"Phase\": Phase information of the alloy in this paper with possible values: 'BCC', 'FCC', 'Im', 'FCC + BCC', 'BCC + Im', 'FCC + Im', 'FCC + BCC + Im'\n",
    "\n",
    "    Return a list of JSON objects if there are several alloys. Only return valid JSON objects, with no additional text.\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "{text}\n",
    "‚Äî‚Äî‚Äî‚Äî‚Äî\n",
    "\"\"\"\n",
    "\n",
    "def main(model, folder, output_dir, max_len, quantize):\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.bfloat16,\n",
    "            \"load_in_4bit\": quantize == \"4bit\",\n",
    "            \"load_in_8bit\": quantize == \"8bit\",\n",
    "        },\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    results = \"{}/{}-{}-{}\".format(output_dir, model.split(\"/\")[-1], quantize, max_len)\n",
    "    print(f\"Results will be saved in {results}\")\n",
    "    if not os.path.exists(results):\n",
    "        os.makedirs(results)\n",
    "\n",
    "    # Step 1: Get all .mmd files in the folder\n",
    "    paper_list = [f[:-4] for f in os.listdir(folder) if f.endswith(\".mmd\")]\n",
    "\n",
    "    # Step 2: Loop through them\n",
    "    for mmd in tqdm(sorted(paper_list)):\n",
    "        print(\"Processing:\", mmd)\n",
    "        mmd_path = os.path.join(folder, mmd + \".mmd\")\n",
    "\n",
    "        if not os.path.exists(mmd_path):\n",
    "            print(\"File not found:\", mmd_path)\n",
    "            continue\n",
    "\n",
    "        with open(mmd_path, \"r\") as f:\n",
    "            ocr = f.read()\n",
    "            paper = split(generator.tokenizer, ocr, max_len)\n",
    "\n",
    "        outputs = []\n",
    "        for part in paper:\n",
    "            dialog = [{\"role\": \"user\", \"content\": prompt_1(part)}]\n",
    "            response = clean(chat(generator, dialog))\n",
    "\n",
    "            outputs.append(response)\n",
    "            del dialog, response\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        merged = []\n",
    "        for o in outputs:\n",
    "            merged.extend(json.loads(o))\n",
    "        unique = [\n",
    "            dict(t) for t in {\n",
    "                tuple(sorted((k, tuple(v) if isinstance(v, list) else v) for k, v in d.items()))\n",
    "                for d in merged\n",
    "            }\n",
    "        ]\n",
    "        with open(os.path.join(results, mmd), \"w\") as f:\n",
    "            f.write(json.dumps(unique, indent=2))\n",
    "\n",
    "max_len = int(max_len)\n",
    "print(\"Files to process:\", os.listdir(input_folder))\n",
    "main(llm_model, input_folder, output_dir, max_len, quantize)\n"
   ],
   "metadata": {
    "id": "hTWKWLOFBWkD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üìÑ Preview Extracted Alloy data\n",
    "# @markdown This cell reads and displays the extracted alloy information.\n",
    "\n",
    "output_file = \"alloy-papers/output/Llama-3.2-3B-Instruct--120000/dummy_alloy_paper\"  # @param {type:\"string\"}\n",
    "\n",
    "import json, pandas as pd\n",
    "with open(output_file, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pd.DataFrame(data)\n"
   ],
   "metadata": {
    "id": "FU_cvX4DchT0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# [Skip] Utilities"
   ],
   "metadata": {
    "id": "eQ4dY-6Ode8Z"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üßπ Free Up GPU Memory\n",
    "# @markdown This cell clears unused variables and empties the GPU memory cache.\n",
    "# @markdown Run this if you experience CUDA out-of-memory errors or after large model inference.\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "id": "5GCnMKtYbjxR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title üßπ Free Up Hugging Face & PyTorch Cache\n",
    "# @markdown This cell clears cache downloaded by Hugging Face (e.g., LLM models) and/or PyTorch (e.g., Nougat models).\n",
    "# @markdown Tick one or both boxes and run the cell to clean them.\n",
    "\n",
    "clean_huggingface_cache = True  # @param {type:\"boolean\"}\n",
    "clean_torch_cache = True        # @param {type:\"boolean\"}\n",
    "\n",
    "if clean_huggingface_cache:\n",
    "    print(\"Cleaning Hugging Face cache...\")\n",
    "    !rm -rf /root/.cache/huggingface\n",
    "    print(\"‚úÖ Hugging Face cache removed.\")\n",
    "\n",
    "if clean_torch_cache:\n",
    "    print(\"Cleaning PyTorch cache...\")\n",
    "    import torch, gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"‚úÖ PyTorch cache cleared.\")\n",
    "\n",
    "if not clean_huggingface_cache and not clean_torch_cache:\n",
    "    print(\"‚ö†Ô∏è Nothing selected to clean.\")\n"
   ],
   "metadata": {
    "id": "5857UQrgQNrn"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
